#!/usr/bin/python

# Copyright 2010-2011 True Blue Logic Ltd
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of version 3 of the GNU General Public License as
# published by the Free Software Foundation.
# 
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
# 
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import binascii, collections, errno, hashlib, itertools, fcntl, os, os.path
import select, stat, string, sqlite3, subprocess, sys, tempfile, time

import synctus.ddar_pb2, synctus.dds
import synctus.netstring as netstring

DEFAULT_REMOTE_PIPELINE_SIZE = '8' # string for cmdline equiv.

# Protocol magic and version exchange is as follows:
#  1. Send magic
#  2. Send my version
#  3. Await magic and version from other side
#  4. Send the version we want to use
#  5. Receive the version the other side wants to use
#  6. Use the lower of the two requested version numbers if possible, or
#     bomb out if not possible
#
# For passive mode, await magic from step 3 moves to step 0.

PROTOCOL_MAGIC = "ddar"
PROTOCOL_VERSION = "1" # ASCII decimal string for readability

def _sysread(fileobj, bufsize=4096):
    '''Read up to bufsize bytes, whatever is available, blocking until at least
    something is available. This is different from fileobj.read() because it
    will only read what is available, rather than waiting until the full data
    is read. Non-blocking mode is awkward because if stdin and stdout are the
    same socket then we have to deal with non-blocking writes, too, so we don't
    use it, and in fact assume that the fileobj is blocking.'''
    fd = fileobj.fileno()
    while True:
        try:
            data = os.read(fd, bufsize)
        except IOError, e:
            if e.errno != errno.EINTR:
                raise
        else:
            return data

def _check_protocol(ipc, passive=False):
    decoder = netstring.Decoder()

    def _send(m):
        ipc.out_f.write(netstring.encode(m))
        ipc.out_f.flush()

    def _read_one_netstring():
        # Read one byte at a time to avoid reading too much and making
        # the API too complicated by having to return excess read data.
        data = None
        while True:
            for s in decoder.feed(data):
                return s
            data = _sysread(ipc.in_f, 1)
            if data == '':
                raise ConsoleError('remote process closed unexpectedly')

    def _check(m):
        magic = _read_one_netstring()
        if magic != m:
            print >>sys.stderr, "%r != %r" % (magic, m)
            raise RuntimeError('Protocol mismatch')

    def _ignore():
        _read_one_netstring()

    if passive:
        _check(PROTOCOL_MAGIC)
        _send(PROTOCOL_MAGIC)
        _send(PROTOCOL_VERSION)
    else:
        _send(PROTOCOL_MAGIC)
        _send(PROTOCOL_VERSION)
        _check(PROTOCOL_MAGIC)

    _ignore() # don't care what other side says; we only support version 1
    _send(PROTOCOL_VERSION)
    _check(PROTOCOL_VERSION)

class ConsoleError(RuntimeError):
    def __init__(self, m):
        self.message = m

class _ImmediateRequest(object):
    def __init__(self, reply):
        self.reply = reply

    def flush(self): pass

class _WorkPipeline(object):
    def __init__(self, size, in_fn, out_fn):
        self.size = size
        self.in_fn = in_fn
        self.out_fn = out_fn

        self.q = collections.deque()

    def _push(self, item):
        self.q.append(self.in_fn(item))
    
    def _pop(self):
        self.out_fn(*self.q.popleft())

    def _flush(self):
        while self.q:
            self._pop()

    def feed_and_flush(self, src):
        '''Given an iterable src, this will call work = in_fn(item) and then
        out_fn(*work) for item in src, interleaving the calls such that size
        work is in progress at a time.'''

        src = iter(src)

        # Fill the pipeline
        for i in xrange(self.size):
            try:
                item = src.next()
            except StopIteration:
                # Source emptied before pipeline filled; just flush the
                # remaining items back out
                self._flush()
                return
            self._push(item)

        # Work the pipeline
        for item in src:
            self._push(item)
            self._pop()

        # Source is now empty: flush the final items
        self._flush()

class Archive(object):
    def __init__(self, dirname, auto_create=False):
        self.dirname = dirname

        if not os.path.exists(self.dirname):
            if auto_create:
                self._create()
            else:
                raise ConsoleError("archive %s not found" % self.dirname)
        elif not os.path.isdir(self.dirname):
            raise ConsoleError("%s exists but is not a ddar directory" %
                               self.dirname)

        try:
            format_name = self._read_small_file(self._format_filename('name'))
        except:
            raise ConsoleError('%s is not a ddar directory' % self.dirname)
        else:
            if format_name.lstrip().rstrip() != 'ddar':
                raise ConsoleError('%s is not a ddar directory' % self.dirname)

        version = self._read_small_file(self._format_filename('version'))
        version = version.lstrip().rstrip()
        if version != '1':
            raise ConsoleError('%s uses ddar archive version %s but only' +
                               'version 1 is supported' % self.dirname)

        self.db = sqlite3.connect(os.path.join(self.dirname, 'db'))
        self.db.text_factory = str
        self.db.execute('PRAGMA foreign_keys = ON')

    @staticmethod
    def _read_small_file(name, size_limit=1024):
        f = open(name, 'r')
        try:
            data = f.read(size_limit)
        finally:
            f.close()
        return data

    @staticmethod
    def _write_small_file(name, data):
        f = open(name, 'w')
        try:
            f.write(data)
        finally:
            f.close()

    def _create(self):
        os.mkdir(self.dirname)
        os.mkdir(os.path.join(self.dirname, 'format'))
        os.mkdir(os.path.join(self.dirname, 'objects'))

        self._write_small_file(self._format_filename('name'), "ddar\n")
        self._write_small_file(self._format_filename('version'), "1\n")

        db = sqlite3.connect(os.path.join(self.dirname, 'db'))
        c = db.cursor()
        c.execute('''
CREATE TABLE member (id INTEGER PRIMARY KEY,
                      name TEXT UNIQUE NOT NULL,
                      length INTEGER,
                      hash BLOB,
                      create_time INTEGER)''')
        c.execute('''
CREATE TABLE chunk (member_id INTEGER NOT NULL,
                    hash BLOB NOT NULL,
                    offset INTEGER NOT NULL,
                    length INTEGER NOT NULL,
                    UNIQUE (member_id, offset),
                    FOREIGN KEY (member_id) REFERENCES member(id))''')
        # No need for a (tag,offset) index as we have a UNIQUE
        # constraint on it. This causes sqlite to generate an index and
        # the documentatation states that there is a performance degradation
        # by having an extra index instead of relying on this fact.
        c.execute('CREATE INDEX chunk_hash_idx ON chunk(hash)')
        c.execute('CREATE INDEX member_create_time_idx ON member(create_time)')
        c.close()
        db.commit()
        db.close()

    def _object_filename(self, h):
        h = binascii.hexlify(h)
        return os.path.join(self.dirname, 'objects', h[0:2], h[2:])

    def _makedirs(self, object_filename):
        try:
            os.makedirs(os.path.dirname(object_filename))
        except OSError, e:
            if e.errno != errno.EEXIST:
                raise

    def _removedirs(self, object_filename):
        try:
            os.removedirs(os.path.dirname(object_filename))
        except OSError, e:
            if e.errno != errno.ENOTEMPTY:
                raise

    def _format_filename(self, n):
        return os.path.join(self.dirname, 'format', n)

    def close(self): pass

    @staticmethod
    def _have_chunk(cursor, h):
        h_blob = buffer(h) # buffer to make sqlite use a BLOB
        cursor.execute('SELECT 1 FROM chunk WHERE hash=? LIMIT 1', (h_blob,))
        return _ImmediateRequest(bool(cursor.fetchone()))

    def _store_chunk(self, member_id, cursor, data, offset, length,
                     sha256=None):
        '''Store the chunk in the database and the object store if necessary,
        but do not commit. The database insert is done last, so that if
        interrupted the database is never wrong. At worst a dangling object
        will be left in the object store.
        
        If sha256 is provided, then data can be None, in which case the chunk
        must be in the object store already. If sha256 and data are both
        provided, then they must match.'''

        if data is None:
            assert(sha256 is not None)
            h = sha256
        else:
            h = hashlib.sha256(data).digest()
            assert(sha256 is None or sha256 == h)
            assert(len(data) == length)

        if not self._have_chunk(cursor, h).reply:
            assert(data is not None)
            object_filename = self._object_filename(h)
            object_dir = os.path.dirname(object_filename)
            self._makedirs(object_filename)
            temp_fd, temp_name = tempfile.mkstemp(dir=object_dir)
            temp = os.fdopen(temp_fd, 'w')
            try:
                temp.write(data)
            finally:
                temp.close()
            os.rename(temp_name, object_filename)

        h_blob = buffer(h)
        cursor.execute('INSERT INTO chunk ' +
                       '(member_id, hash, offset, length) ' +
                       'VALUES (?, ?, ?, ?)',
                       (member_id, h_blob, offset, length))

    @staticmethod
    def _store_add_member(cursor, tag):
        cursor.execute('SELECT 1 FROM member WHERE name=? LIMIT 1', (tag,))
        if cursor.fetchone():
            raise ConsoleError("member %s already exists" % tag)

        cursor.execute('INSERT INTO member (name, create_time) VALUES (?, ?)',
                       (tag, int(time.time())))
        member_id = cursor.lastrowid
        cursor.execute('DELETE FROM chunk WHERE member_id=?', (member_id,))
        return member_id

    def _store_complete_member(self, cursor, h, length, member_id):
        h_blob = buffer(h)
        cursor.execute('UPDATE member SET hash=?, length=? WHERE ' +
                       'id=?', (h_blob, length, member_id))
        return _ImmediateRequest(None)

    def _store_commit(self, cursor):
        try:
            cursor.close()
        except:
            pass
        self.db.commit()

    def store_server(self, ipc, tag):
        cursor = self.db.cursor()
        member_id = self._store_add_member(cursor, tag)
        server = self._StoreRPCServer(archive=self,
                                      ipc=ipc,
                                      member_id=member_id,
                                      cursor=cursor)
        result = server.loop()
        server.close()
        return result

    def store(self, tag, f=sys.stdin, aio=False, pipeline_size=None):
        cursor = self.db.cursor()
        member_id = self._store_add_member(cursor, tag)
        try:
            self._store(cursor, member_id, f, aio,
                        pipeline_size=pipeline_size)
        finally:
            self._store_commit(cursor)

    def _analyze_and_store(self, cursor, dds, member_id, pipeline_size=None):
        if pipeline_size is None:
            pipeline_size = 0

        total_length = [0]
        full_h = hashlib.sha256()

        def in_fn(data):
            offset = total_length[0]
            length = len(data)
            chunk_h = hashlib.sha256(data).digest()

            request = self._have_chunk(cursor=cursor, h=chunk_h)

            # Update running stats
            total_length[0] += length
            full_h.update(data)
            
            return request, data, chunk_h, offset, length

        def out_fn(request, data, chunk_h, offset, length):
            if request.reply:
                data = None
            self._store_chunk(member_id=member_id,
                              cursor=cursor,
                              data=data,
                              offset=offset,
                              length=length,
                              sha256=chunk_h)

        work_pipeline = _WorkPipeline(pipeline_size, in_fn, out_fn)
        work_pipeline.feed_and_flush(dds.chunks())
            
        return total_length[0], full_h.digest()

    def _store(self, cursor, member_id, f, aio, pipeline_size=None):
        dds = synctus.dds.DDS()
        dds.set_file(f)
        if aio:
            dds.set_aio()
        dds.begin()

        length, h = self._analyze_and_store(cursor, dds, member_id,
                                            pipeline_size=pipeline_size)
        
        self._store_complete_member(cursor=cursor,
                                    h=h,
                                    length=length,
                                    member_id=member_id).flush()

    class _StoreRPCServer(object):
        def __init__(self, archive, ipc, member_id, cursor):
            self.archive = archive
            self.ipc = ipc
            self.member_id = member_id
            self.cursor = cursor

        def _rpc_have_chunk_request(self, req):
            reply = synctus.ddar_pb2.HaveChunkReply()
            reply.have = self.archive._have_chunk(self.cursor, req.sha256).reply
            reply.sha256 = req.sha256
            return reply

        def _rpc_store_chunk_request(self, req):
            if req.HasField('data'):
                data = req.data
            else:
                data = None

            self.archive._store_chunk(member_id=self.member_id,
                                      cursor=self.cursor,
                                      data=data,
                                      offset=req.offset,
                                      length=req.length,
                                      sha256=req.sha256)
            reply = synctus.ddar_pb2.StoreChunkReply()
            reply.sha256 = req.sha256
            return reply

        def _rpc_commit_request(self, req):
            self.archive._store_complete_member(cursor=self.cursor,
                                                h=req.sha256,
                                                length=req.length,
                                                member_id=self.member_id)
            # Commit now so that it is confirmed before the reply. Another
            # attempt will happen after EOF as well.
            self.archive._store_commit(self.cursor)
            reply = synctus.ddar_pb2.CommitReply()
            reply.sha256 = req.sha256
            return reply

        def _rpc_request(self, name, req):
            reply = getattr(self, '_rpc_' + name, req)(req)
            wrapped_reply = synctus.ddar_pb2.Reply()
            for field in wrapped_reply.DESCRIPTOR.fields:
                if field.message_type == reply.DESCRIPTOR:
                    getattr(wrapped_reply, field.name).MergeFrom(reply)

                    # SetInParent is not present in google.protobuf 2.0.3 and
                    # so breaks Hardy. This isn't actually needed right now
                    # as all our protocol messages have content so will get
                    # set automatically

                    # getattr(wrapped_reply, field.name).SetInParent()
                    break
            else:
                raise RuntimeError("Couldn't find reply field in wrapper")
            self.ipc.out_f.write(netstring.encode(wrapped_reply.SerializeToString()))
            self.ipc.out_f.flush()

        def loop(self):
            _check_protocol(self.ipc)

            decoder = netstring.Decoder()

            try:
                data = _sysread(self.ipc.in_f)
                while data:
                    for encoded_request in decoder.feed(data):
                        request_container = synctus.ddar_pb2.Request()
                        request_container.ParseFromString(encoded_request)
                        for req_type, req in request_container.ListFields():
                            self._rpc_request(req_type.name, req)

                    data = _sysread(self.ipc.in_f)
            finally:
                # Same as local: commit anyway to get any partial writes. This
                # will be consistent since we are careful about storing chunk
                # object files before adding them to the DB
                
                # This could have also happened on CommitRequest but a
                # duplicate is fine
                self.archive._store_commit(self.cursor)

        def close(self):
            self.ipc.close()

    def load(self, tag, f=sys.stdout):
        cursor = self.db.cursor()
        cursor.execute('SELECT id, hash FROM member WHERE name=?', (tag,))
        row = cursor.fetchone()
        if not row:
            raise ConsoleError('member %s not found in archive' % tag)

        member_id, h = row
        expected_h = str(h)
        cursor.execute('SELECT hash, offset, length ' +
                       'FROM chunk WHERE member_id=? ' +
                       'ORDER BY offset', (member_id,))

        h2 = hashlib.sha256()
        next_offset = 0
        row = cursor.fetchone()
        while row:
            h, offset, length = row
            assert(offset == next_offset)
            g = open(self._object_filename(h), 'rb')
            try:
                data = g.read()
            finally:
                g.close()
            assert(len(data) == length)
            h2.update(data)
            f.write(data)

            next_offset = offset + length
            row = cursor.fetchone()

        if expected_h != h2.digest():
            raise ConsoleError('extracted member failed hash check')

    def delete(self, tag):
        cursor = self.db.cursor()
        cursor2 = self.db.cursor()
        cursor.execute('SELECT id FROM member WHERE name=?', (tag,))
        row = cursor.fetchone()
        if not row:
            raise ConsoleError('member %s not found in archive' % tag)
        member_id = row[0]
        cursor.execute('SELECT hash FROM chunk WHERE member_id=?',
                       (member_id,))
        row = cursor.fetchone()
        while row:
            h = row[0]
            cursor2.execute('SELECT 1 FROM chunk WHERE hash=? AND ' +
                            'member_id != ? LIMIT 1', (h, member_id))
            if not cursor2.fetchone():
                try:
                    object_filename = self._object_filename(h)
                    os.unlink(object_filename)
                    self._removedirs(object_filename)
                except OSError, e:
                    # ignore ENOENT to make delete idempotent on a SIGINT
                    if e.errno != errno.ENOENT:
                        raise
            row = cursor.fetchone()
        cursor.execute('DELETE FROM chunk WHERE member_id=?', (member_id,))
        cursor.execute('DELETE FROM member WHERE id=?', (member_id,))
        cursor.close()
        self.db.commit()

    def list_tags(self):
        cursor = self.db.cursor()
        cursor.execute('SELECT name FROM member')
        tag = cursor.fetchone()
        while tag:
            yield tag[0]
            tag = cursor.fetchone()

    def get_last_tag(self):
        cursor = self.db.cursor()
        cursor.execute('SELECT name FROM member ORDER BY rowid DESC LIMIT 1')
        row = cursor.fetchone()
        return row[0] if row else None

    def suggest_tag(self):
        cursor = self.db.cursor()
        base = time.strftime('%Y-%m-%d')
        cursor.execute('SELECT 1 FROM member WHERE name=?', (base,))
        if not cursor.fetchone():
            return base
        root_like = base + '-%'
        cursor.execute('SELECT COUNT(*) FROM member WHERE name LIKE ?',
                       (root_like,))
        row = cursor.fetchone()
        if not row or not row[0]:
            return '%s-2' % base

        suffix = row[0] + 1

        cursor.execute('SELECT 1 FROM member WHERE name=?',
                       ('%s-%s' % (base, suffix),))
        row = cursor.fetchone()
        while row:
            suffix += 1
            cursor.execute('SELECT 1 FROM member WHERE name=?',
                           ('%s-%s' % (base, suffix),))
            row = cursor.fetchone()

        return '%s-%s' % (base, suffix)

    hexdigits = set(string.hexdigits) - set(string.uppercase)
    sha256_length = len(hashlib.sha256().hexdigest())

    @classmethod
    def _valid_hex_hash(cls, h):
        return (len(h) == cls.sha256_length and
            all((d in cls.hexdigits for d in h)))

    def _fsck_fs(self):
        status = True

        cursor = self.db.cursor()
        for h in os.listdir(os.path.join(self.dirname, 'objects')):
            if not self._valid_hex_hash(h):
                continue
            cursor.execute('SELECT 1 FROM chunk WHERE hash=? LIMIT 1',
                           (buffer(binascii.unhexlify(h)),))
            if not cursor.fetchone():
                print 'Unknown object %s' % h
                status = False

        return status

    def _fsck_db_to_fs_chunk(self):
        status = True

        cursor = self.db.cursor()

        # Check each hash is correct
        cursor.execute('SELECT DISTINCT hash, length FROM chunk')

        row = cursor.fetchone()
        while row:
            h, length = row
            h = str(h) # sqlite3 returns a buffer for a BLOB; we want an str;
                       # otherwise comparisons never match
            try:
                f = open(self._object_filename(h), 'rb')
                try:
                    data = f.read(length+1)
                finally:
                    f.close()
            except IOError:
                print "Could not read chunk %s" % binascii.hexlify(h)
                status = False
            else:
                if len(data) != length:
                    print "Chunk %s wrong size" % binascii.hexlify(h)
                    status = False
                elif hashlib.sha256(data).digest() != h:
                    print "Chunk %s corrupt" % binascii.hexlify(h)
                    status = False
            
            row = cursor.fetchone()

        return status

    def _fsck_db_to_fs_member(self):
        status = True

        cursor = self.db.cursor()
        cursor2 = self.db.cursor()
        # Check each member hash is correct
        cursor.execute('SELECT id, name, hash FROM member')
        row = cursor.fetchone()
        while row:
            member_id, tag, h = row
            h = str(h)

            cursor2.execute('SELECT hash FROM chunk WHERE member_id=? ' +
                            'ORDER BY offset', (member_id,))

            h2 = hashlib.sha256()
            row2 = cursor2.fetchone()
            while row2:
                chunk_hash = row2[0]
                try:
                    f = open(self._object_filename(chunk_hash), 'rb')
                    try:
                        data = f.read()
                    finally:
                        f.close()
                except IOError:
                    print ("Could not read chunk %s from %s" %
                            (binascii.hexlify(chunk_hash), tag))
                    status = False
                else:
                    h2.update(data)

                row2 = cursor2.fetchone()

            if h != h2.digest():
                print '%s: hash mismatch' % tag
                status = False

            row = cursor.fetchone()
        
        return status

    def _fsck_db(self):
        status = True

        cursor = self.db.cursor()
        cursor2 = self.db.cursor()

        # Check through each member in the chunk table
        cursor.execute('SELECT member_id, offset, length FROM chunk ' +
                       'ORDER BY member_id, offset')
        current_member_id = None
        row = cursor.fetchone()
        while row:
            member_id, offset, length = row
            if member_id != current_member_id:
                current_member_id = member_id
                current_offset = 0
                tag = cursor2.execute('SELECT name FROM member WHERE id=?',
                                      (member_id,)).fetchone()[0]
            if offset != current_offset:
                if offset < current_offset:
                    print ("Chunk at offset %d for %s has an overlap" %
                        (offset, tag))
                    status = False
                else:
                    assert(offset > current_offset)
                    print ("Hole in %s found between %d and %d" %
                        (tag, current_offset, offset))
                    status = False
                current_offset = offset
            current_offset += length

            row = cursor.fetchone()

        # Check all lengths in member match chunk totals
        cursor.execute('''
SELECT member.name

FROM
    member,

    (SELECT member_id, sum(length) AS length
     FROM chunk GROUP BY member_id) AS chunk_lengths

WHERE
      member.id = chunk_lengths.member_id
      AND member.length != chunk_lengths.length''')
        
        row = cursor.fetchone()
        while row:
            print 'Length mismatch in %s' % row[0]
            status = False
            row = cursor.fetchone()

        return status

    def fsck(self):
        status = True
        status = status and self._fsck_db()
        status = status and self._fsck_fs()
        status = status and self._fsck_db_to_fs_chunk()
        status = status and self._fsck_db_to_fs_member()
        return status

    def print_sha256sum(self, tags):
        def _print_row(row):
            # Double space to match sha256sum output format
            if row[0] is None:
                print '-  %s' % row[1]
            else:
                print '%s  %s' % (binascii.hexlify(str(row[0])), row[1])

        cursor = self.db.cursor()
        if tags:
            for tag in tags:
                cursor.execute('SELECT hash, name FROM member WHERE name=? LIMIT 1', (tag,))
                row = cursor.fetchone()
                if not row:
                    raise ConsoleError('member not found: %s' % tag)
                _print_row(row)
        else:
            cursor.execute('SELECT hash, name FROM member')
            row = cursor.fetchone()
            while row:
                _print_row(row)
                row = cursor.fetchone()

class RemoteArchive(Archive):
    def __init__(self, ipc):
        # Override parent completely
        self.ipc = ipc

        self.decoder = netstring.Decoder()
        self.request_q = collections.deque()

        _check_protocol(self.ipc, passive=True)

    def _not_implemented(self):
        raise NotImplementedError()

    store = _not_implemented
    load = _not_implemented
    delete = _not_implemented
    list_tags = _not_implemented
    get_last_tag = _not_implemented
    suggest_tag = _not_implemented
    fsck = _not_implemented

    def close(self):
        self.ipc.close()

    def _read_one_netstring(self):
        data = None
        while True:
            for s in self.decoder.feed(data):
                return s
            data = _sysread(self.ipc.in_f)
            if data == '':
                raise ConsoleError('remote process closed unexpectedly')

    class _RemoteArchiveReply(object):
        def __init__(self, remote_archive, callback):
            self.remote_archive = remote_archive
            self.callback = callback
            self._have_reply = False

        @property
        def reply(self):
            if not self._have_reply:
                self.flush()
            return self._reply

        def receive_reply(self, value):
            self._reply = self.callback(value)
            self._have_reply = True

        def flush(self):
            if self._have_reply:
                return
            self.remote_archive._read_reply(self)
            assert(self._have_reply)

    def _read_one_reply(self):
        request = self.request_q.popleft()
        reply = synctus.ddar_pb2.Reply()
        reply.ParseFromString(self._read_one_netstring())
        request.receive_reply(reply)
        return request

    def _read_reply(self, request):
        while self._read_one_reply() is not request: pass

    def _request(self, request, callback):
        enc_req = netstring.encode(request.SerializeToString())
        try:
            self.ipc.out_f.write(enc_req)
            self.ipc.out_f.flush()
        except IOError, e:
            if e.errno == errno.EPIPE:
                raise ConsoleError('remote process closed unexpectedly')
            else:
                raise
        request = self._RemoteArchiveReply(self, callback)
        self.request_q.append(request)
        return request

    def _have_chunk(self, cursor, h):
        request = synctus.ddar_pb2.Request()
        request.have_chunk_request.sha256 = h

        def _process_have_chunk_reply(reply):
            assert(reply.HasField('have_chunk_reply'))
            assert(reply.have_chunk_reply.sha256 == h)
            return reply.have_chunk_reply.have

        return self._request(request, _process_have_chunk_reply)

    def _store_chunk(self, member_id, cursor, data, offset, length,
                     sha256=None):
        if sha256 is None:
            sha256 = hashlib.sha256(data).digest()
        request = synctus.ddar_pb2.Request()
        r = request.store_chunk_request
        if data is not None:
            r.data = data
        r.sha256 = sha256
        r.offset = offset
        r.length = length

        def _process_store_chunk_reply(reply):
            assert(reply.HasField('store_chunk_reply'))
            assert(reply.store_chunk_reply.sha256 == sha256)

        return self._request(request, _process_store_chunk_reply)

    def _store_complete_member(self, cursor, h, length, member_id):
        request = synctus.ddar_pb2.Request()
        request.commit_request.sha256 = h
        request.commit_request.length = length

        def _process_store_complete_member_reply(reply):
            assert(reply.HasField('commit_reply'))

        return self._request(request, _process_store_complete_member_reply)

    def store(self, tag, f=sys.stdin, aio=False, server=False,
              pipeline_size=None):
        assert(not server)
        self._store(None, None, f, aio, pipeline_size=pipeline_size)

class OptionError(RuntimeError):
    def __init__(self, m):
        self.message = m

class OptionHelpRequest(Exception): pass

def parse_args(args, pos_arg_names=None, bool_options=None, arg_options=None,
               exclusive_options=None):
    # The argument parser options are modelled after GNU ar, tar, gzip and
    # tarsnap. The aim is to not confuse or annoy anyone already familiar with
    # those tools by behaving as one would expect. Unfortunately I can't seem
    # to find a way to make this happen using argparse, optparse or getopt, so
    # doing it by hand it is.

    result = {}
    pos_args = []
    pos_arg_names = pos_arg_names or []
    bool_options = bool_options or set()
    arg_options = arg_options or set()
    exclusive_options = exclusive_options or set()

    all_options = (bool_options | arg_options |
                   set(itertools.chain(*exclusive_options)))
    single_letter_options = set([x for x in all_options if len(x) == 1])
    long_options = set([x for x in all_options if len(x) > 1])

    def set_result(k, v):
        for excl_set in exclusive_options:
            if k in excl_set and any((k in result for k in excl_set)):
                option_list = ', '.join(('-' if len(opt) == 1 else '--') + opt for opt in excl_set)
                raise OptionError('only one of %s may be specified' % option_list)
        result[k] = v

    def parse_long_option(arg_iter, arg):
        arg = arg.lstrip('-')

        if arg == 'help':
            raise OptionHelpRequest()

        idx = arg.find('=')
        if idx >= 0:
            equals_param = arg[idx+1:]
            arg = arg[:idx]
            if arg not in arg_options:
                raise OptionError('option %s does not take a parameter' % arg)
        else:
            equals_param = None

        if arg in arg_options:
            if equals_param is None:
                set_result(arg, arg_iter.next())
            else:
                set_result(arg, equals_param)
        elif arg in bool_options:
            set_result(arg, True)
        else:
            raise OptionError('unknown option: %s' % arg)

    def parse_option_cluster(arg_iter, arg):
        arg = arg.lstrip('-')
        while arg:
            if arg[0] in single_letter_options:
                if arg[0] in arg_options:
                    if len(arg) == 1:
                        set_result(arg[0], arg_iter.next())
                    else:
                        set_result(arg[0], arg[1:])
                    return
                elif arg[0] in bool_options:
                    set_result(arg[0], True)
                    arg = arg[1:]
                    continue
                else:
                    raise NotImplementedError()
            elif arg[0] == 'h':
                raise OptionHelpRequest()
            else:
                raise OptionError('unknown option: %s' % arg[0])

    arg_iter = iter(args)
    first = True
    while True:
        try:
            arg = arg_iter.next()
        except StopIteration:
            break
        if first and not arg.startswith('--'):
            # First arg is always the command even without a '-', like ps
            # and tar do it, unless it is a long option
            parse_option_cluster(arg_iter, arg)
        elif arg == '--':
            # Remaining args are all positional
            pos_args.extend(arg_iter)
            break
        elif not arg:
            # An empty arg? I guess it's positional
            pos_args.append(arg)
        elif arg == '-':
            # This is a positional arg too
            pos_args.append(arg)
        elif arg.startswith('--'):
            parse_long_option(arg_iter, arg)
        elif arg.startswith('-'):
            parse_option_cluster(arg_iter, arg)
        else:
            pos_args.append(arg)
        first = False

    result.update(zip(pos_arg_names, pos_args))

    last_pos_arg_name = pos_arg_names[-1]
    try:
        result[last_pos_arg_name] = [ result[last_pos_arg_name] ]
    except KeyError:
        result[last_pos_arg_name] = []
    result[last_pos_arg_name].extend(pos_args[len(pos_arg_names):])

    for opt in list(all_options) + pos_arg_names:
        if opt not in result:
            result[opt] = None

    return result

def main_add_one(store, filename, tag, ipc=None, pipeline_size=None):
    if ipc:
        store.store_server(ipc=ipc, tag=tag)
    elif filename == '-':
        store.store(tag, sys.stdin, pipeline_size=pipeline_size)
    else:
        if filename[0] == '!':
            filename = filename[1:]
            p = subprocess.Popen([filename], stdout=subprocess.PIPE,
                                 close_fds=True, shell=True)
            f = p.stdout
            aio = False
            def close():
                result = p.wait()
                if result:
                    raise ConsoleError('process %s returned %s' %
                                            (filename, result))
        else:
            f = open(filename, 'rb')
            aio = True
            def close():
                f.close()
        try:
            store.store(tag, f, aio=aio, pipeline_size=pipeline_size)
        finally:
            close()

def main_add(store, members, tag=None, ipc=None, pipeline_size=None):
    if not members:
        if not tag:
            try: tag = store.suggest_tag()
            except NotImplementedError: pass
        main_add_one(store, '-', tag, ipc=ipc,
                     pipeline_size=pipeline_size)
    elif len(members) == 1:
        if not tag:
            tag = members[0]
        main_add_one(store, members[0], tag, ipc=ipc,
                     pipeline_size=pipeline_size)
    else:
        for member in members:
            main_add_one(store, member, member, ipc=ipc,
                         pipeline_size=pipeline_size)

def main_extract(store, members):
    if not members:
        members = [ store.get_last_tag() ]
    for tag in members:
        store.load(tag, sys.stdout)

class RshIPC(object):
    def __init__(self, cmd, host, args):
        remote_args = [ cmd, host, "ddar" ]
        remote_args.append(' '.join(args))
        self._p = subprocess.Popen(remote_args,
                                   stdin=subprocess.PIPE,
                                   stdout=subprocess.PIPE,
                                   shell=False,
                                   close_fds=True)

        self.in_f = self._p.stdout
        self.out_f = self._p.stdin
    
    def close(self):
        self.out_f.close()
        result = self._p.wait()
        if result != 0:
            raise ConsoleError('remote process returned %d' % result)

class StdIPC(object):
    '''IPC using stdin and stdout (eg. we were called by a remote process via
    ssh)'''
    def __init__(self):
        self.in_f = sys.stdin
        self.out_f = sys.stdout

    def close(self):
        '''Close will happen when process exits'''
        pass

ddar_arg_spec = {
    'pos_arg_names': [ 'member' ],
    'bool_options': set('ctxd') | set([ 'fsck', 'force-stdout', 'server',
                                        'sender', 'sha256sum' ]),
    'arg_options': set([ 'f', 'N', 'rsh', 'pipeline-size' ]),
    'exclusive_options': set([frozenset([ 'c', 't', 'x', 'd', 'fsck',
                                          'sha256sum' ])])
}

def main():
    try:
        args = parse_args(sys.argv[1:], **ddar_arg_spec)
        if not any((args[k] for k in (list('cxtd') + ['fsck', 'sha256sum']))):
            raise OptionError('a command is required')
        if not args['f'] and not args['sender']:
            try:
                args['f'] = args['member'].pop(0)
            except IndexError:
                raise OptionError('an archive must be specified')
        if args['d'] and not args['member']:
            raise OptionError('no member specified')
        if args['N'] and not args['c']:
            raise OptionError('option -N not valid except in create mode')
        if args['server'] and args['sender']:
            raise OptionError('--server and --sender cannot both be set')

        if len(args['member']) > 1 and (':' in args['f'] or args['server']):
            raise OptionError('can only add one item at once to remote archive')
        if not args['sender'] and any(x[0] == '!' for x in args['member']):
            raise OptionError('shell out is for remote sources only; use stdin')

        if args['rsh']:
            rsh = args['rsh']
        else:
            rsh = 'ssh'

        if args['sender']:
            archive = RemoteArchive(StdIPC())
            source_ipc = None
            if args['pipeline-size'] is None:
                args['pipeline-size'] = DEFAULT_REMOTE_PIPELINE_SIZE
        elif ':' in args['f']:
            if not args['c']:
                raise OptionError('remote archive only permitted with -c')
            host, filename = args['f'].split(':')
            remote_args = [ '--server', '-c', '-f', filename ]
            if args['N']:
                remote_args.extend(['-N', args['N']])
            remote_args.extend(args['member'])
            archive_ipc = RshIPC(cmd=rsh, host=host, args=remote_args)
            archive = RemoteArchive(archive_ipc)
            args['f'] = filename
            if args['pipeline-size'] is None:
                args['pipeline-size'] = DEFAULT_REMOTE_PIPELINE_SIZE
            source_ipc = None
        elif args['member'] and any(':' in m for m in args['member']):
            if not args['c']:
                raise OptionError('remote source only permitted with -c')
            if len(args['member']) > 1:
                raise OptionError('only one member permitted when using ' +
                                  'remote source')
            if not args['c']:
                raise OptionError('remote archive only permitted with -c')

            if args['pipeline-size'] is None:
                args['pipeline-size'] = '0'
            archive = Archive(args['f'], auto_create=True)
            
            host, filename = args['member'][0].split(':')
            if filename[0] == '!':
                args['member'] = []
            else:
                args['member'][0] = filename
            remote_args = [ '--sender', '-c', filename ]
            if args['N']:
                remote_args.extend(['-N', args['N']])
            source_ipc = RshIPC(cmd=rsh, host=host, args=remote_args)
        else:
            source_ipc = StdIPC() if args['server'] else None
            archive = Archive(args['f'], auto_create=args['c'])
            if args['pipeline-size'] is None:
                args['pipeline-size'] = '0'

        if args['c']:
            main_add(archive, args['member'], args['N'], ipc=source_ipc,
                     pipeline_size=int(args['pipeline-size']))
        elif args['x']:
            if not args['force-stdout'] and os.isatty(sys.stdout.fileno()):
                raise OptionError('output is a terminal and --force-stdout not specified')
            main_extract(archive, args['member'])
        elif args['d']:
            for member in args['member']:
                archive.delete(member)
        elif args['t']:
            for tag in archive.list_tags():
                print tag
        elif args['fsck']:
            if not archive.fsck():
                archive.close()
                print 'fsck returned errors'
                sys.exit(1)
        elif args['sha256sum']:
            archive.print_sha256sum(args['member'])

        archive.close()

    except ConsoleError, e:
        print >>sys.stderr, 'ddar:', e.message
        sys.exit(1)

    except OptionHelpRequest:
        print '''ddar: store multiple files efficiently in a de-duplicated archive

Create or add to an archive:
    ddar [-]c [-f] archive member [member...]
    ddar [-]c [-f] [server:]archive [-N member-name] < file
    ddar [-]c [-f] [server:]archive [-N member-name] member
    ddar [-]c [-f] archive server:member [-N member-name]
    ddar [-]c [-f] archive server:!cmd [-N member-name]

    If member-name is unspecified, ddar will use the supplied filename
    as the member name, or if none is available then it will create a
    suitable name based on the current date.

Extract from an archive:
    ddar [-]x [options] [-f] archive > file  # extract the most recent member
    ddar [-]x [options] [-f] archive member-name > file

    Options:
        --force-stdout  Write to stdout even if stdout is a terminal

List members in an archive:
    ddar [-]t [-f] archive

Delete members from an archive:
    ddar [-]d [-f] archive member-name [member-name...]

Check an archive for integrity:
    ddar --fsck [-f] archive


Examples:

  Back up your home directory daily:

    To an external disk:

      tar c ~|gzip --rsyncable|ddar cf /mnt/external_disk/home_backup


    To a remote server (using ssh, with ddar also installed remotely):

      tar c ~|gzip --rsyncable|ddar cf server:home_backup


  Back up your home directory on a remote server to a local machine daily:

      ddar cf server_home_backup server:\!"'tar c ~|gzip --rsyncable'"


  Restore your home directory after a disaster:

      ddar xf /mnt/external_disk/home_backup|tar xzC/
'''
        return
    except OptionError, e:
        print >>sys.stderr, 'ddar: %s' % e.message
        print >>sys.stderr, 'Try: ddar --help'
        sys.exit(2)

if __name__ == '__main__':
    main()

# vim: set ts=8 sts=4 sw=4 ai et :
